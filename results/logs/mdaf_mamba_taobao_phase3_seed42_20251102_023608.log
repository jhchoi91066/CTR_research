2025-11-02 02:36:08,266 [INFO] ================================================================================
2025-11-02 02:36:08,267 [INFO] PHASE 3: ENHANCED REGULARIZATION - MDAF_MAMBA_TAOBAO
2025-11-02 02:36:08,267 [INFO] ================================================================================
2025-11-02 02:36:08,267 [INFO] 
Configuration:
2025-11-02 02:36:08,267 [INFO]   Model: mamba
2025-11-02 02:36:08,267 [INFO]   Epochs: 15
2025-11-02 02:36:08,267 [INFO]   Batch Size: 2048
2025-11-02 02:36:08,267 [INFO]   Learning Rate: 0.0003
2025-11-02 02:36:08,267 [INFO]   Dropout: 0.3
2025-11-02 02:36:08,267 [INFO]   Weight Decay: 5e-05
2025-11-02 02:36:08,267 [INFO]   Label Smoothing: 0.1
2025-11-02 02:36:08,267 [INFO]   Gradient Clip Norm: 1.0
2025-11-02 02:36:08,267 [INFO]   Warmup Epochs: 2
2025-11-02 02:36:08,267 [INFO]   Early Stopping Patience: 5
2025-11-02 02:36:08,267 [INFO]   Random Seed: 42
2025-11-02 02:36:08,267 [INFO] 
1. Loading Taobao metadata...
2025-11-02 02:36:08,267 [INFO]    Vocabulary sizes:
2025-11-02 02:36:08,267 [INFO]       Items: 335,164
2025-11-02 02:36:08,267 [INFO]       Categories: 5,480
2025-11-02 02:36:08,267 [INFO]       Users: 577,482
2025-11-02 02:36:08,267 [INFO]    Max sequence length: 50
2025-11-02 02:36:08,284 [INFO] 
2. Device: mps
2025-11-02 02:36:08,285 [INFO] 
3. Creating dataloaders...
2025-11-02 02:36:09,154 [INFO]    Train samples: 1,052,081
2025-11-02 02:36:09,154 [INFO]    Val samples: 225,446
2025-11-02 02:36:09,154 [INFO]    Train batches: 514
2025-11-02 02:36:09,154 [INFO]    Val batches: 111
2025-11-02 02:36:09,154 [INFO] 
4. Creating MDAF_MAMBA_TAOBAO model...
2025-11-02 02:36:09,821 [INFO]    Total parameters: 45,969,365
2025-11-02 02:36:09,821 [INFO]    Trainable parameters: 45,969,365
2025-11-02 02:36:10,259 [INFO] 
5. Training components:
2025-11-02 02:36:10,259 [INFO]    Optimizer: Adam (lr=0.0003, weight_decay=5e-05)
2025-11-02 02:36:10,259 [INFO]    LR Scheduler: Warmup(2 epochs) + CosineAnnealing
2025-11-02 02:36:10,259 [INFO]    Loss: BCELoss + LabelSmoothing(0.1)
2025-11-02 02:36:10,259 [INFO]    Gradient Clipping: Max Norm = 1.0
2025-11-02 02:36:10,259 [INFO] 
6. Starting training...
2025-11-02 02:36:10,259 [INFO] ================================================================================
2025-11-02 02:36:10,260 [INFO] 
Epoch 1/15
2025-11-02 02:36:10,260 [INFO] --------------------------------------------------------------------------------
2025-11-02 14:04:16,855 [INFO]    Batch [200/514] Loss: 0.3317, LR: 5.84e-05
2025-11-02 14:11:27,276 [INFO]    Batch [400/514] Loss: 0.3059, LR: 1.17e-04
2025-11-02 15:09:53,993 [INFO] 
  Train - Loss: 0.3799, AUC: 0.5121, LogLoss: 0.2939
2025-11-02 15:15:35,374 [INFO]   Val   - Loss: 0.3076, AUC: 0.5700, LogLoss: 0.2008
2025-11-02 15:15:35,376 [INFO]   Gap   - Train-Val AUC: -0.0578
2025-11-02 15:15:35,376 [INFO]   Gate  - Mean: 0.1671, Std: 0.1745, Range: [0.0733, 0.7090]
2025-11-02 15:15:35,376 [INFO]   LR    - 1.500000e-04
2025-11-02 15:15:36,879 [INFO] 
  âœ“ New best model saved! Val AUC: 0.5700
2025-11-02 15:15:36,879 [INFO] 
Epoch 2/15
2025-11-02 15:15:36,880 [INFO] --------------------------------------------------------------------------------
