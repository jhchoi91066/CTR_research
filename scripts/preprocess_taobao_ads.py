"""
Taobao Ad Click ë°ì´í„°ì…‹ ì „ì²˜ë¦¬

ë°ì´í„° êµ¬ì¡°:
- raw_sample.csv: user, time_stamp, adgroup_id, pid, nonclk, clk
- ad_feature.csv: adgroup_id, cate_id, campaign_id, customer, brand, price
- user_profile.csv: userid, cms_segid, cms_group_id, final_gender_code, age_level, pvalue_level, shopping_level, occupation, new_user_class_level

BST ëª¨ë¸ì„ ìœ„í•œ ì‹œí€€ìŠ¤ íŠ¹ì„± ìƒì„±:
- ì‚¬ìš©ìì˜ ê³¼ê±° í´ë¦­í•œ ê´‘ê³  ì‹œí€€ìŠ¤ë¥¼ item_historyë¡œ ì‚¬ìš©
- adgroup_idë¥¼ item_idë¡œ, cate_idë¥¼ category_idë¡œ ì‚¬ìš©
"""

import pandas as pd
import numpy as np
from pathlib import Path
import pickle
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# ê²½ë¡œ ì„¤ì •
data_dir = Path('data/raw/taobao')
output_dir = Path('data/processed/taobao')
output_dir.mkdir(parents=True, exist_ok=True)

print("="*60)
print("Taobao Ad Click ë°ì´í„°ì…‹ ì „ì²˜ë¦¬")
print("="*60)

# 1. raw_sample.csv ë¡œë“œ (ë©”ì¸ ë°ì´í„°)
print("\nğŸ“¥ raw_sample.csv ë¡œë”© ì¤‘...")
df = pd.read_csv(data_dir / 'raw_sample.csv')
print(f"   ì›ë³¸ ë°ì´í„°: {len(df):,} rows")
print(f"   ì»¬ëŸ¼: {df.columns.tolist()}")

# 2. ad_feature.csv ë¡œë“œ (ê´‘ê³  íŠ¹ì„±)
print("\nğŸ“¥ ad_feature.csv ë¡œë”© ì¤‘...")
ad_features = pd.read_csv(data_dir / 'ad_feature.csv')
print(f"   ê´‘ê³  íŠ¹ì„±: {len(ad_features):,} rows")

# 3. user_profile.csv ë¡œë“œ (ì‚¬ìš©ì í”„ë¡œí•„)
print("\nğŸ“¥ user_profile.csv ë¡œë”© ì¤‘...")
user_profile = pd.read_csv(data_dir / 'user_profile.csv')
print(f"   ì‚¬ìš©ì í”„ë¡œí•„: {len(user_profile):,} rows")

# 4. ë°ì´í„° ì¡°ì¸
print("\nğŸ”— ë°ì´í„° ì¡°ì¸ ì¤‘...")
df = df.merge(ad_features, on='adgroup_id', how='left')
df = df.merge(user_profile, left_on='user', right_on='userid', how='left')

print(f"   ì¡°ì¸ í›„: {len(df):,} rows")

# 5. ìƒ˜í”Œë§ (ë„ˆë¬´ í¬ë©´)
SAMPLE_SIZE = 2_000_000  # 200ë§Œê°œë¡œ ì œí•œ
if len(df) > SAMPLE_SIZE:
    print(f"\nâš ï¸  ë°ì´í„°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ ({len(df):,} rows). {SAMPLE_SIZE:,} rowsë¡œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.")
    df = df.sample(n=SAMPLE_SIZE, random_state=42)
    print(f"   ìƒ˜í”Œë§ í›„: {len(df):,} rows")

# 6. íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
df['label'] = df['clk']  # clk = 1ì´ë©´ í´ë¦­, 0ì´ë©´ ë¹„í´ë¦­

print(f"\nğŸ“Š ë ˆì´ë¸” ë¶„í¬:")
print(df['label'].value_counts())
print(f"   CTR: {df['label'].mean():.4f}")

# 7. ì‹œê°„ ìˆœ ì •ë ¬
df = df.sort_values(['user', 'time_stamp'])
print(f"\nğŸ“ˆ ì‹œê°„ ìˆœ ì •ë ¬ ì™„ë£Œ")

# 8. í•„ìˆ˜ ì»¬ëŸ¼ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
print(f"\nğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")
df['cate_id'] = df['cate_id'].fillna(0).astype(int)
df['brand'] = df['brand'].fillna(0).astype(int)
df['price'] = df['price'].fillna(df['price'].median())

# ì‚¬ìš©ì í”„ë¡œí•„ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
df['final_gender_code'] = df['final_gender_code'].fillna(0).astype(int)
df['age_level'] = df['age_level'].fillna(0).astype(int)
df['pvalue_level'] = df['pvalue_level'].fillna(0).astype(int)

# 9. Feature Engineering
print(f"\nğŸ”§ Feature Engineering...")

# LabelEncoderë¡œ ID ì¸ì½”ë”©
le_user = LabelEncoder()
le_adgroup = LabelEncoder()
le_cate = LabelEncoder()

df['user_id_encoded'] = le_user.fit_transform(df['user'])
df['item_id_encoded'] = le_adgroup.fit_transform(df['adgroup_id'])  # adgroup_idë¥¼ item_idë¡œ
df['category_id_encoded'] = le_cate.fit_transform(df['cate_id'])

# ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ
df['hour'] = pd.to_datetime(df['time_stamp'], unit='s').dt.hour
df['dayofweek'] = pd.to_datetime(df['time_stamp'], unit='s').dt.dayofweek

print(f"   ì‚¬ìš©ì ìˆ˜: {df['user_id_encoded'].nunique():,}")
print(f"   ê´‘ê³ ê·¸ë£¹ ìˆ˜: {df['item_id_encoded'].nunique():,}")
print(f"   ì¹´í…Œê³ ë¦¬ ìˆ˜: {df['category_id_encoded'].nunique():,}")

# 10. ì‚¬ìš©ì í–‰ë™ ì‹œí€€ìŠ¤ ìƒì„±
MAX_SEQ_LEN = 50  # ë…¼ë¬¸ ê¶Œì¥ ê¸¸ì´

print(f"\nğŸ“Š ì‚¬ìš©ì í–‰ë™ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘... (MAX_SEQ_LEN={MAX_SEQ_LEN})")

# ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ í•„í„°ë§
user_counts = df.groupby('user_id_encoded').size()
valid_users = user_counts[user_counts >= 3].index
df = df[df['user_id_encoded'].isin(valid_users)]

print(f"   ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ 3 í•„í„°ë§ í›„: {len(df):,} rows")
print(f"   ìœ íš¨ ì‚¬ìš©ì ìˆ˜: {len(valid_users):,}")

# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì‹œí€€ìŠ¤ ìƒì„±
df = df.reset_index(drop=True)

item_histories = []
category_histories = []  # â­ ì¶”ê°€: ì¹´í…Œê³ ë¦¬ íˆìŠ¤í† ë¦¬
print("   íˆìŠ¤í† ë¦¬ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")

for user_id in tqdm(df['user_id_encoded'].unique(), desc="ì‹œí€€ìŠ¤ ìƒì„±"):
    user_mask = df['user_id_encoded'] == user_id
    user_indices = df.index[user_mask].tolist()
    user_items = df.loc[user_mask, 'item_id_encoded'].tolist()
    user_categories = df.loc[user_mask, 'category_id_encoded'].tolist()  # â­ ì¶”ê°€

    for i, idx in enumerate(user_indices):
        # í˜„ì¬ ìƒ˜í”Œ ì´ì „ì˜ í´ë¦­í•œ ì•„ì´í…œë“¤ë§Œ
        hist_seq = user_items[:i][-MAX_SEQ_LEN:]
        hist_cat_seq = user_categories[:i][-MAX_SEQ_LEN:]  # â­ ì¶”ê°€

        # Padding
        if len(hist_seq) < MAX_SEQ_LEN:
            hist_seq = [0] * (MAX_SEQ_LEN - len(hist_seq)) + hist_seq
            hist_cat_seq = [0] * (MAX_SEQ_LEN - len(hist_cat_seq)) + hist_cat_seq  # â­ ì¶”ê°€

        item_histories.append(hist_seq)
        category_histories.append(hist_cat_seq)  # â­ ì¶”ê°€

df['item_history'] = item_histories
df['category_history'] = category_histories  # â­ ì¶”ê°€

# 11. Train/Val/Test ë¶„í• 
print(f"\nâœ‚ï¸  ë°ì´í„° ë¶„í•  ì¤‘...")
train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])

print(f"   Train: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)")
print(f"   Val:   {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)")
print(f"   Test:  {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)")

print(f"\n   Train CTR: {train_df['label'].mean():.4f}")
print(f"   Val CTR:   {val_df['label'].mean():.4f}")
print(f"   Test CTR:  {test_df['label'].mean():.4f}")

# 12. ë©”íƒ€ë°ì´í„° ì €ì¥
metadata = {
    'n_users': df['user_id_encoded'].nunique(),
    'n_items': df['item_id_encoded'].nunique(),
    'n_categories': df['category_id_encoded'].nunique(),
    'max_seq_len': MAX_SEQ_LEN,
    'vocab_sizes': {
        'user_id': df['user_id_encoded'].max() + 1,
        'item_id': df['item_id_encoded'].max() + 1,
        'category_id': df['category_id_encoded'].max() + 1,
        'hour': 24,
        'dayofweek': 7
    }
}

# 13. ì €ì¥
print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")

# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
save_columns = [
    'user_id_encoded', 'item_id_encoded', 'category_id_encoded',
    'hour', 'dayofweek', 'item_history', 'category_history', 'label'  # â­ category_history ì¶”ê°€
]

train_df[save_columns].to_parquet(output_dir / 'train.parquet', index=False)
val_df[save_columns].to_parquet(output_dir / 'val.parquet', index=False)
test_df[save_columns].to_parquet(output_dir / 'test.parquet', index=False)

with open(output_dir / 'metadata.pkl', 'wb') as f:
    pickle.dump(metadata, f)

print(f"   âœ… ì €ì¥ ì™„ë£Œ: {output_dir}")

print("\n" + "="*60)
print("âœ… Taobao Ad Click ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
print("="*60)
print(f"\nğŸ“‚ ì €ì¥ ê²½ë¡œ: {output_dir}")
print(f"   - train.parquet: {len(train_df):,} rows")
print(f"   - val.parquet: {len(val_df):,} rows")
print(f"   - test.parquet: {len(test_df):,} rows")
print(f"   - metadata.pkl")
print(f"\nğŸ“Š ë©”íƒ€ë°ì´í„°:")
for key, value in metadata.items():
    if key != 'vocab_sizes':
        print(f"   {key}: {value}")
    else:
        print(f"   vocab_sizes:")
        for k, v in value.items():
            print(f"      {k}: {v}")
