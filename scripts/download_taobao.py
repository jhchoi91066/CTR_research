"""
Taobao UserBehavior ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬

ë°ì´í„°ì…‹: https://tianchi.aliyun.com/dataset/dataDetail?dataId=649
ë˜ëŠ” Kaggle: https://www.kaggle.com/datasets/pavansanagapati/ad-displayclick-data-on-taobaocom

ë°ì´í„° êµ¬ì¡°:
- user_id: ì‚¬ìš©ì ID
- item_id: ìƒí’ˆ ID
- category_id: ì¹´í…Œê³ ë¦¬ ID
- behavior_type: í–‰ë™ íƒ€ì… (pv, buy, cart, fav)
- timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
"""

import pandas as pd
import numpy as np
from pathlib import Path
import pickle
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# ê²½ë¡œ ì„¤ì •
data_dir = Path('data/raw/taobao')
output_dir = Path('data/processed/taobao')
output_dir.mkdir(parents=True, exist_ok=True)

print("="*60)
print("Taobao UserBehavior ë°ì´í„°ì…‹ ì „ì²˜ë¦¬")
print("="*60)

# ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì „ í…ŒìŠ¤íŠ¸ìš©)
print("\nğŸ“¥ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")

# Taobao í˜•ì‹ì˜ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
np.random.seed(42)

n_samples = 100000
n_users = 1000
n_items = 5000
n_categories = 100

# ì‚¬ìš©ì í–‰ë™ ì‹œí€€ìŠ¤ ìƒì„±
data = {
    'user_id': np.random.randint(1, n_users+1, n_samples),
    'item_id': np.random.randint(1, n_items+1, n_samples),
    'category_id': np.random.randint(1, n_categories+1, n_samples),
    'behavior_type': np.random.choice(['pv', 'cart', 'fav', 'buy'], n_samples, p=[0.7, 0.1, 0.1, 0.1]),
    'timestamp': np.random.randint(1511539200, 1512057600, n_samples)  # 2017-11-25 ~ 2017-12-01
}

df = pd.DataFrame(data)
df = df.sort_values(['user_id', 'timestamp'])

print(f"   ìƒ˜í”Œ ë°ì´í„°: {len(df):,} rows")
print(f"   ì‚¬ìš©ì ìˆ˜: {df['user_id'].nunique():,}")
print(f"   ìƒí’ˆ ìˆ˜: {df['item_id'].nunique():,}")

# íƒ€ê²Ÿ ìƒì„±: buy í–‰ë™ì„ 1ë¡œ, ë‚˜ë¨¸ì§€ë¥¼ 0ìœ¼ë¡œ
df['label'] = (df['behavior_type'] == 'buy').astype(int)

print(f"\nğŸ“Š ë ˆì´ë¸” ë¶„í¬:")
print(df['label'].value_counts())
print(f"   CTR: {df['label'].mean():.4f}")

# ì‚¬ìš©ìë³„ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚°
user_seq_lengths = df.groupby('user_id').size()
print(f"\nğŸ“ˆ ì‚¬ìš©ìë³„ í–‰ë™ ì‹œí€€ìŠ¤ í†µê³„:")
print(f"   í‰ê· : {user_seq_lengths.mean():.1f}")
print(f"   ì¤‘ì•™ê°’: {user_seq_lengths.median():.1f}")
print(f"   ìµœëŒ€: {user_seq_lengths.max()}")

# Feature Engineering
print(f"\nğŸ”§ Feature Engineering...")

# LabelEncoderë¡œ ID ì¸ì½”ë”©
le_user = LabelEncoder()
le_item = LabelEncoder()
le_category = LabelEncoder()

df['user_id_encoded'] = le_user.fit_transform(df['user_id'])
df['item_id_encoded'] = le_item.fit_transform(df['item_id'])
df['category_id_encoded'] = le_category.fit_transform(df['category_id'])

# ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ
df['hour'] = pd.to_datetime(df['timestamp'], unit='s').dt.hour
df['dayofweek'] = pd.to_datetime(df['timestamp'], unit='s').dt.dayofweek

# ì‚¬ìš©ì í–‰ë™ ì‹œí€€ìŠ¤ íŠ¹ì„± (ê°„ë‹¨ ë²„ì „)
# ê° ì‚¬ìš©ìì˜ ìµœê·¼ Nê°œ ì•„ì´í…œ IDë¥¼ ì‹œí€€ìŠ¤ë¡œ ì €ì¥
MAX_SEQ_LEN = 10

user_item_sequences = {}
user_category_sequences = {}

for user_id in tqdm(df['user_id_encoded'].unique(), desc="ì‹œí€€ìŠ¤ ìƒì„±"):
    user_data = df[df['user_id_encoded'] == user_id].sort_values('timestamp')
    item_seq = user_data['item_id_encoded'].tolist()
    cat_seq = user_data['category_id_encoded'].tolist()

    user_item_sequences[user_id] = item_seq
    user_category_sequences[user_id] = cat_seq

# ê° ìƒ˜í”Œì— ëŒ€í•´ íˆìŠ¤í† ë¦¬ ì‹œí€€ìŠ¤ ì¶”ê°€
def get_history_sequence(row_idx, user_id, max_len=MAX_SEQ_LEN):
    """ì´ì „ í–‰ë™ ì‹œí€€ìŠ¤ ë°˜í™˜"""
    user_seq = user_item_sequences[user_id]
    user_idx = list(df[df['user_id_encoded'] == user_id].index).index(row_idx)

    # í˜„ì¬ ìƒ˜í”Œ ì´ì „ì˜ í–‰ë™ë“¤
    hist_seq = user_seq[:user_idx][-max_len:]

    # Padding
    if len(hist_seq) < max_len:
        hist_seq = [0] * (max_len - len(hist_seq)) + hist_seq

    return hist_seq

print("   íˆìŠ¤í† ë¦¬ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
df['item_history'] = [
    get_history_sequence(idx, row['user_id_encoded'])
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="íˆìŠ¤í† ë¦¬")
]

# Train/Val/Test ë¶„í• 
print(f"\nâœ‚ï¸  ë°ì´í„° ë¶„í•  ì¤‘...")
train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])

print(f"   Train: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)")
print(f"   Val:   {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)")
print(f"   Test:  {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)")

# ë©”íƒ€ë°ì´í„° ì €ì¥
metadata = {
    'n_users': df['user_id_encoded'].nunique(),
    'n_items': df['item_id_encoded'].nunique(),
    'n_categories': df['category_id_encoded'].nunique(),
    'max_seq_len': MAX_SEQ_LEN,
    'vocab_sizes': {
        'user_id': df['user_id_encoded'].max() + 1,
        'item_id': df['item_id_encoded'].max() + 1,
        'category_id': df['category_id_encoded'].max() + 1,
        'hour': 24,
        'dayofweek': 7
    }
}

# ì €ì¥
print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")
train_df.to_parquet(output_dir / 'train.parquet', index=False)
val_df.to_parquet(output_dir / 'val.parquet', index=False)
test_df.to_parquet(output_dir / 'test.parquet', index=False)

with open(output_dir / 'metadata.pkl', 'wb') as f:
    pickle.dump(metadata, f)

print(f"   âœ… ì €ì¥ ì™„ë£Œ: {output_dir}")

print("\n" + "="*60)
print("âœ… Taobao ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
print("="*60)
print(f"\nğŸ“‚ ì €ì¥ ê²½ë¡œ: {output_dir}")
print(f"   - train.parquet: {len(train_df):,} rows")
print(f"   - val.parquet: {len(val_df):,} rows")
print(f"   - test.parquet: {len(test_df):,} rows")
print(f"   - metadata.pkl")
print(f"\nğŸ“Š ë©”íƒ€ë°ì´í„°:")
for key, value in metadata.items():
    if key != 'vocab_sizes':
        print(f"   {key}: {value}")
    else:
        print(f"   vocab_sizes:")
        for k, v in value.items():
            print(f"      {k}: {v}")
