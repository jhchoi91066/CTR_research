"""
Taobao UserBehavior ì‹¤ì œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬

ë°ì´í„°ì…‹ ì¶œì²˜:
1. Kaggle: https://www.kaggle.com/datasets/pavansanagapati/ad-displayclick-data-on-taobaocom
2. Tianchi: https://tianchi.aliyun.com/dataset/dataDetail?dataId=649

ë°ì´í„° êµ¬ì¡°:
- user_id: ì‚¬ìš©ì ID
- item_id: ìƒí’ˆ ID
- category_id: ì¹´í…Œê³ ë¦¬ ID
- behavior_type: í–‰ë™ íƒ€ì… (pv, buy, cart, fav)
- timestamp: íƒ€ì„ìŠ¤íƒ¬í”„

ì‚¬ìš©ë²•:
1. Kaggle CLIë¡œ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥):
   kaggle datasets download -d pavansanagapati/ad-displayclick-data-on-taobaocom

2. ë˜ëŠ” ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í›„ data/raw/taobao/ ì— ì €ì¥
"""

import pandas as pd
import numpy as np
from pathlib import Path
import pickle
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import os

# ê²½ë¡œ ì„¤ì •
data_dir = Path('data/raw/taobao')
output_dir = Path('data/processed/taobao')
output_dir.mkdir(parents=True, exist_ok=True)

print("="*60)
print("Taobao UserBehavior ì‹¤ì œ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬")
print("="*60)

# ì‹¤ì œ ë°ì´í„° íŒŒì¼ í™•ì¸
raw_file_candidates = [
    data_dir / 'UserBehavior.csv',
    data_dir / 'taobao_data.csv',
    data_dir / 'user_behavior.csv',
]

raw_file = None
for candidate in raw_file_candidates:
    if candidate.exists():
        raw_file = candidate
        print(f"\nâœ… ë°ì´í„° íŒŒì¼ ë°œê²¬: {raw_file}")
        break

if raw_file is None:
    print("\nâš ï¸  ì‹¤ì œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("\në‹¤ìŒ ë°©ë²•ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
    print("\n1. Kaggle CLI ì‚¬ìš© (ê¶Œì¥):")
    print("   kaggle datasets download -d pavansanagapati/ad-displayclick-data-on-taobaocom")
    print("   unzip ad-displayclick-data-on-taobaocom.zip -d data/raw/taobao/")
    print("\n2. ë˜ëŠ” https://www.kaggle.com/datasets/pavansanagapati/ad-displayclick-data-on-taobaocom")
    print("   ì—ì„œ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í›„ data/raw/taobao/ ì— ì €ì¥")

    # ì„ì‹œë¡œ ì†Œê·œëª¨ ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©
    print("\nâš ï¸  ì„ì‹œë¡œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    print("   (ì‹¤ì œ ì„±ëŠ¥ ê²€ì¦ì„ ìœ„í•´ì„œëŠ” ì‹¤ì œ ë°ì´í„° ì‚¬ìš© í•„ìš”)")

    exec(open('scripts/download_taobao.py').read())
    exit(0)

# ì‹¤ì œ ë°ì´í„° ë¡œë“œ
print(f"\nğŸ“¥ ë°ì´í„° ë¡œë”© ì¤‘... (ì´ ê³¼ì •ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

# Taobao ë°ì´í„°ëŠ” ë³´í†µ ì»¬ëŸ¼ëª…ì´ ì—†ê±°ë‚˜ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
# ì¼ë°˜ì ì¸ í˜•ì‹: user_id, item_id, category_id, behavior_type, timestamp
try:
    df = pd.read_csv(raw_file, header=None, names=['user_id', 'item_id', 'category_id', 'behavior_type', 'timestamp'])
except:
    # í—¤ë”ê°€ ìˆëŠ” ê²½ìš°
    df = pd.read_csv(raw_file)

print(f"   ì›ë³¸ ë°ì´í„°: {len(df):,} rows")
print(f"   ì»¬ëŸ¼: {df.columns.tolist()}")

# ë°ì´í„° ìƒ˜í”Œë§ (ì „ì²´ ë°ì´í„°ê°€ ë„ˆë¬´ í° ê²½ìš°)
SAMPLE_SIZE = 1_000_000  # 100ë§Œê°œë¡œ ì œí•œ (í•„ìš”ì‹œ ì¡°ì •)
if len(df) > SAMPLE_SIZE:
    print(f"\nâš ï¸  ë°ì´í„°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ ({len(df):,} rows). {SAMPLE_SIZE:,} rowsë¡œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.")
    df = df.sample(n=SAMPLE_SIZE, random_state=42)
    print(f"   ìƒ˜í”Œë§ í›„: {len(df):,} rows")

# ì •ë ¬
df = df.sort_values(['user_id', 'timestamp'])

print(f"\nğŸ“Š ë°ì´í„° ê¸°ë³¸ í†µê³„:")
print(f"   ì‚¬ìš©ì ìˆ˜: {df['user_id'].nunique():,}")
print(f"   ìƒí’ˆ ìˆ˜: {df['item_id'].nunique():,}")
print(f"   ì¹´í…Œê³ ë¦¬ ìˆ˜: {df['category_id'].nunique():,}")

# íƒ€ê²Ÿ ìƒì„±: buy í–‰ë™ì„ 1ë¡œ, ë‚˜ë¨¸ì§€ë¥¼ 0ìœ¼ë¡œ
df['label'] = (df['behavior_type'] == 'buy').astype(int)

print(f"\nğŸ“Š ë ˆì´ë¸” ë¶„í¬:")
print(df['label'].value_counts())
print(f"   CTR: {df['label'].mean():.4f}")

# ì‚¬ìš©ìë³„ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚°
user_seq_lengths = df.groupby('user_id').size()
print(f"\nğŸ“ˆ ì‚¬ìš©ìë³„ í–‰ë™ ì‹œí€€ìŠ¤ í†µê³„:")
print(f"   í‰ê· : {user_seq_lengths.mean():.1f}")
print(f"   ì¤‘ì•™ê°’: {user_seq_lengths.median():.1f}")
print(f"   ìµœëŒ€: {user_seq_lengths.max()}")

# ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ í•„í„°ë§ (ë„ˆë¬´ ì§§ì€ ì‹œí€€ìŠ¤ ì œê±°)
MIN_SEQ_LEN = 3
user_counts = df.groupby('user_id').size()
valid_users = user_counts[user_counts >= MIN_SEQ_LEN].index
df = df[df['user_id'].isin(valid_users)]

print(f"\nğŸ”§ ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ {MIN_SEQ_LEN} í•„í„°ë§ í›„: {len(df):,} rows")

# Feature Engineering
print(f"\nğŸ”§ Feature Engineering...")

# LabelEncoderë¡œ ID ì¸ì½”ë”©
le_user = LabelEncoder()
le_item = LabelEncoder()
le_category = LabelEncoder()

df['user_id_encoded'] = le_user.fit_transform(df['user_id'])
df['item_id_encoded'] = le_item.fit_transform(df['item_id'])
df['category_id_encoded'] = le_category.fit_transform(df['category_id'])

# ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ
df['hour'] = pd.to_datetime(df['timestamp'], unit='s').dt.hour
df['dayofweek'] = pd.to_datetime(df['timestamp'], unit='s').dt.dayofweek

# ì‚¬ìš©ì í–‰ë™ ì‹œí€€ìŠ¤ íŠ¹ì„±
# ê° ì‚¬ìš©ìì˜ ìµœê·¼ Nê°œ ì•„ì´í…œ IDë¥¼ ì‹œí€€ìŠ¤ë¡œ ì €ì¥
MAX_SEQ_LEN = 50  # ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ ê¸¸ì´

print(f"   ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´: {MAX_SEQ_LEN}")

# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì‹œí€€ìŠ¤ ìƒì„±
print("   íˆìŠ¤í† ë¦¬ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
df = df.reset_index(drop=True)

item_histories = []
for user_id in tqdm(df['user_id_encoded'].unique(), desc="ì‹œí€€ìŠ¤ ìƒì„±"):
    user_mask = df['user_id_encoded'] == user_id
    user_indices = df.index[user_mask].tolist()
    user_items = df.loc[user_mask, 'item_id_encoded'].tolist()

    for i, idx in enumerate(user_indices):
        # í˜„ì¬ ìƒ˜í”Œ ì´ì „ì˜ í–‰ë™ë“¤
        hist_seq = user_items[:i][-MAX_SEQ_LEN:]

        # Padding
        if len(hist_seq) < MAX_SEQ_LEN:
            hist_seq = [0] * (MAX_SEQ_LEN - len(hist_seq)) + hist_seq

        item_histories.append(hist_seq)

df['item_history'] = item_histories

# Train/Val/Test ë¶„í• 
print(f"\nâœ‚ï¸  ë°ì´í„° ë¶„í•  ì¤‘...")
train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])

print(f"   Train: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)")
print(f"   Val:   {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)")
print(f"   Test:  {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)")

# ë©”íƒ€ë°ì´í„° ì €ì¥
metadata = {
    'n_users': df['user_id_encoded'].nunique(),
    'n_items': df['item_id_encoded'].nunique(),
    'n_categories': df['category_id_encoded'].nunique(),
    'max_seq_len': MAX_SEQ_LEN,
    'vocab_sizes': {
        'user_id': df['user_id_encoded'].max() + 1,
        'item_id': df['item_id_encoded'].max() + 1,
        'category_id': df['category_id_encoded'].max() + 1,
        'hour': 24,
        'dayofweek': 7
    }
}

# ì €ì¥
print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")
train_df.to_parquet(output_dir / 'train.parquet', index=False)
val_df.to_parquet(output_dir / 'val.parquet', index=False)
test_df.to_parquet(output_dir / 'test.parquet', index=False)

with open(output_dir / 'metadata.pkl', 'wb') as f:
    pickle.dump(metadata, f)

print(f"   âœ… ì €ì¥ ì™„ë£Œ: {output_dir}")

print("\n" + "="*60)
print("âœ… Taobao ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
print("="*60)
print(f"\nğŸ“‚ ì €ì¥ ê²½ë¡œ: {output_dir}")
print(f"   - train.parquet: {len(train_df):,} rows")
print(f"   - val.parquet: {len(val_df):,} rows")
print(f"   - test.parquet: {len(test_df):,} rows")
print(f"   - metadata.pkl")
print(f"\nğŸ“Š ë©”íƒ€ë°ì´í„°:")
for key, value in metadata.items():
    if key != 'vocab_sizes':
        print(f"   {key}: {value}")
    else:
        print(f"   vocab_sizes:")
        for k, v in value.items():
            print(f"      {k}: {v}")
